{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Projeto CriaComp"
      ],
      "metadata": {
        "id": "5g-DgVun7VeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliotecas e Instalações"
      ],
      "metadata": {
        "id": "mv5SEqfd7SKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 0: Instalar bibliotecas e baixar o modelo de forma OTIMIZADA\n",
        "\n",
        "# 1. Bibliotecas para a interface e visualização\n",
        "!pip install -q gradio supervision\n",
        "\n",
        "# 2. Biblioteca principal do Segment Anything 2 (SAM)\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything-2.git\n",
        "\n",
        "# 3. Bibliotecas para a comparação de texto (Prática Cognitiva)\n",
        "!pip install -q fuzzywuzzy python-Levenshtein\n",
        "\n",
        "# 4. Bibliotecas para a comparação de texto (OpenAI)\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMejS4-gEzpk",
        "outputId": "b964d9a0-dc55-41e0-a851-91b17a489cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo a chave da OpenAI\n",
        "from google.colab import userdata\n",
        "userdata.get('OPENAI_API_KEY')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "iwOp3fFtz1NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando bibliotecas importantes\n",
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import openai"
      ],
      "metadata": {
        "id": "h5vYzTRvk4ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o caminho do arquivo do modelo\n",
        "checkpoint_path = \"checkpoints/sam2_hiera_large.pt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Cria o diretório se não existir\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Verifica se o modelo já foi baixado e se é válido pelo tamanho\n",
        "file_is_valid = False\n",
        "if os.path.exists(checkpoint_path):\n",
        "    # Um arquivo válido tem mais de 2GB. Um arquivo menor está corrompido.\n",
        "    try:\n",
        "        file_size_gb = os.path.getsize(checkpoint_path) / (1024**3)\n",
        "        if file_size_gb > 2.0:\n",
        "            print(f\"Modelo encontrado com tamanho válido ({file_size_gb:.2f} GB). Download pulado.\")\n",
        "            file_is_valid = True\n",
        "        else:\n",
        "            print(f\"Modelo encontrado, mas o tamanho ({file_size_gb:.2f} GB) é muito pequeno. Removendo arquivo corrompido.\")\n",
        "            os.remove(checkpoint_path) # Remove o arquivo inválido\n",
        "    except OSError as e:\n",
        "        print(f\"Erro ao acessar o arquivo: {e}. Removendo para tentar novamente.\")\n",
        "        os.remove(checkpoint_path)\n",
        "\n",
        "if not file_is_valid:\n",
        "    print(f\"Iniciando download do modelo para '{checkpoint_path}'...\")\n",
        "    # Baixa o modelo mostrando o progresso\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -O {checkpoint_path}\n",
        "    print(\"Download concluído.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RvPRf8qj8ug",
        "outputId": "fdbc454f-cf20-49e4-a33c-8ef7dc112863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo encontrado, mas o tamanho (0.84 GB) é muito pequeno. Removendo arquivo corrompido.\n",
            "Iniciando download do modelo para 'checkpoints/sam2_hiera_large.pt'...\n",
            "--2025-08-13 16:49:07--  https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.96, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 897952466 (856M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘checkpoints/sam2_hiera_large.pt’\n",
            "\n",
            "checkpoints/sam2_hi 100%[===================>] 856.35M  88.0MB/s    in 34s     \n",
            "\n",
            "2025-08-13 16:49:42 (24.9 MB/s) - ‘checkpoints/sam2_hiera_large.pt’ saved [897952466/897952466]\n",
            "\n",
            "Download concluído.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Outras bibliotecas importantes\n",
        "import gradio as gr\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import shutil\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator"
      ],
      "metadata": {
        "id": "YnHG5z4ukNeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração do Modelo"
      ],
      "metadata": {
        "id": "9IcD_Ldp7PwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURAÇÃO DO MODELO (Executado apenas uma vez) ---\n",
        "print(\"Carregando o modelo SAM 2. Isso pode levar um momento...\")\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_PATH = \"checkpoints/sam2_hiera_large.pt\"\n",
        "CONFIG_PATH = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "# Verifica se o arquivo de configuração existe, se não, cria um básico.\n",
        "if not os.path.exists(CONFIG_PATH):\n",
        "    with open(CONFIG_PATH, 'w') as f:\n",
        "        f.write(\"# Arquivo de configuração para SAM 2\\n\")\n",
        "\n",
        "sam2_model = build_sam2(CONFIG_PATH, CHECKPOINT_PATH, device=DEVICE)\n",
        "\n",
        "# O gerador de máscaras é a principal ferramenta para segmentação automática.\n",
        "mask_generator = SAM2AutomaticMaskGenerator(\n",
        "    model=sam2_model, #modelo usado\n",
        "    points_per_side=32, #Mais pontos = mais precisão, mas também maior custo computacional e maior detalhamento\n",
        "    pred_iou_thresh=0.86, #mínimo de qualidade para manter uma máscara gerada\n",
        "    stability_score_thresh=0.90, #filtram máscaras ruins\n",
        "    crop_n_layers=0, #melhora a qualidade com múltiplas escalas, mas 0 é mais rápido\n",
        "    min_mask_region_area=2000 #remove ruídos pequenos\n",
        ")\n",
        "print(\"Modelo carregado com sucesso!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM59X689bBUE",
        "outputId": "f4a90cbd-6c50-48b0-957e-7df0a6c7a4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o modelo SAM 2. Isso pode levar um momento...\n",
            "Modelo carregado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação da Aplicação"
      ],
      "metadata": {
        "id": "q_a73Mnu7dg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ARMAZENAMENTO DAS ANOTAÇÕES E DADOS GLOBAIS ---\n",
        "ANNOTATIONS = {}\n",
        "MASKS_CACHE = {}\n",
        "TEMP_DIR = \"segmentos_temp\"\n",
        "\n",
        "# --- LÓGICA DA APLICAÇÃO ---\n",
        "\n",
        "if os.path.exists(TEMP_DIR):\n",
        "    shutil.rmtree(TEMP_DIR)\n",
        "os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "\n",
        "def segment_image(image_pil):\n",
        "    global ANNOTATIONS, MASKS_CACHE\n",
        "    if image_pil is None:\n",
        "        return [], None, None, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    print(\"Iniciando segmentação...\")\n",
        "    ANNOTATIONS.clear()\n",
        "    MASKS_CACHE.clear()\n",
        "    if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)\n",
        "    os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "\n",
        "    image_rgb = image_pil.convert(\"RGB\")\n",
        "    masks = mask_generator.generate(np.array(image_rgb))\n",
        "\n",
        "    if not masks:\n",
        "        return [], None, None, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "    sorted_masks = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
        "    image_rgba_np = np.array(image_pil.convert(\"RGBA\"))\n",
        "    segment_paths = []\n",
        "\n",
        "    for i, mask_data in enumerate(sorted_masks):\n",
        "        filename = f\"segmento_{i}.png\"\n",
        "        filepath = os.path.join(TEMP_DIR, filename)\n",
        "        MASKS_CACHE[filename] = mask_data['segmentation']\n",
        "        segment_img_np = image_rgba_np.copy()\n",
        "        segment_img_np[~mask_data['segmentation']] = [0, 0, 0, 0]\n",
        "        coords = np.argwhere(mask_data['segmentation'])\n",
        "        y0, x0 = coords.min(axis=0)\n",
        "        y1, x1 = coords.max(axis=0) + 1\n",
        "        cropped_segment = segment_img_np[y0:y1, x0:x1]\n",
        "        Image.fromarray(cropped_segment).save(filepath)\n",
        "        segment_paths.append(filepath)\n",
        "\n",
        "    return segment_paths, segment_paths, image_pil, gr.update(visible=True), gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "def update_overlay_image(original_image_pil):\n",
        "    if original_image_pil is None: return None\n",
        "    overlay_img = np.array(original_image_pil.convert(\"RGB\"))\n",
        "    for filename, data in ANNOTATIONS.items():\n",
        "        if data.get(\"valid\") and filename in MASKS_CACHE:\n",
        "            mask = MASKS_CACHE[filename]\n",
        "            color = np.random.randint(0, 255, 3)\n",
        "            overlay_img[mask] = overlay_img[mask] * 0.5 + color * 0.5\n",
        "    return Image.fromarray(overlay_img.astype(np.uint8))\n",
        "\n",
        "def get_annotation_data(evt: gr.SelectData, all_paths):\n",
        "    selected_path = all_paths[evt.index]\n",
        "    filename = os.path.basename(selected_path)\n",
        "    existing_annotation = ANNOTATIONS.get(filename, {\"valid\": False, \"description\": \"\"})\n",
        "    is_valid = existing_annotation[\"valid\"]\n",
        "    return selected_path, filename, filename, is_valid, existing_annotation[\"description\"], gr.update(visible=is_valid)\n",
        "\n",
        "def handle_valid_checkbox(is_valid, filename, original_image):\n",
        "    global ANNOTATIONS\n",
        "    if not filename: return gr.update(visible=False), \"Selecione um segmento primeiro.\", original_image\n",
        "    if is_valid: return gr.update(visible=True), \"Adicione uma descrição e salve.\", original_image\n",
        "    else:\n",
        "        status_message = f\"Segmento '{filename}' descartado.\" if filename in ANNOTATIONS else \"Segmento não estava salvo.\"\n",
        "        if filename in ANNOTATIONS: del ANNOTATIONS[filename]\n",
        "        return gr.update(visible=False), status_message, update_overlay_image(original_image)\n",
        "\n",
        "def save_annotation(original_filename, new_filename_text, description, original_image):\n",
        "    global ANNOTATIONS, MASKS_CACHE\n",
        "    if not original_filename: return \"Erro: Nenhum segmento selecionado.\", None, None, original_image\n",
        "    new_filename = new_filename_text if new_filename_text.lower().endswith('.png') else new_filename_text + '.png'\n",
        "    if original_filename != new_filename:\n",
        "        original_path, new_path = os.path.join(TEMP_DIR, original_filename), os.path.join(TEMP_DIR, new_filename)\n",
        "        if os.path.exists(new_path): return f\"Erro: O nome de arquivo '{new_filename}' já existe.\", None, None, original_image\n",
        "        try:\n",
        "            os.rename(original_path, new_path)\n",
        "            if original_filename in ANNOTATIONS: del ANNOTATIONS[original_filename]\n",
        "            if original_filename in MASKS_CACHE: MASKS_CACHE[new_filename] = MASKS_CACHE.pop(original_filename)\n",
        "        except OSError as e: return f\"Erro ao renomear arquivo: {e}\", None, None, original_image\n",
        "    ANNOTATIONS[new_filename] = {\"valid\": True, \"description\": description}\n",
        "    updated_paths = sorted([os.path.join(TEMP_DIR, f) for f in os.listdir(TEMP_DIR)])\n",
        "    return f\"Anotação salva para '{new_filename}'!\", updated_paths, updated_paths, update_overlay_image(original_image)\n",
        "\n",
        "def finalize_annotations():\n",
        "    global MASKS_CACHE\n",
        "    all_files_on_disk = os.listdir(TEMP_DIR)\n",
        "    valid_filenames = list(ANNOTATIONS.keys())\n",
        "    deleted_count = 0\n",
        "    for filename in all_files_on_disk:\n",
        "        if filename not in valid_filenames:\n",
        "            try:\n",
        "                os.remove(os.path.join(TEMP_DIR, filename))\n",
        "                if filename in MASKS_CACHE: del MASKS_CACHE[filename]\n",
        "                deleted_count += 1\n",
        "            except OSError as e: print(f\"Erro ao deletar arquivo inválido {filename}: {e}\")\n",
        "    status = f\"Finalizado! {deleted_count} segmentos inválidos foram apagados.\"\n",
        "    return gr.update(visible=True), gr.update(visible=True), gr.update(choices=valid_filenames, value=None), gr.update(choices=valid_filenames, value=None), status\n",
        "\n",
        "def export_annotations_to_json():\n",
        "    json_data = {fn: data[\"description\"] for fn, data in ANNOTATIONS.items() if data.get(\"valid\")}\n",
        "    if not json_data: return None, None\n",
        "    json_path = \"annotations.json\"\n",
        "    with open(json_path, 'w', encoding='utf-8') as f: json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
        "    return json_path, json_data\n",
        "\n",
        "def show_and_get_description(filename):\n",
        "    if not filename: return None, \"Nenhuma descrição.\"\n",
        "    image_path = os.path.join(TEMP_DIR, filename)\n",
        "    description = ANNOTATIONS.get(filename, {}).get(\"description\", \"Nenhuma descrição válida encontrada.\")\n",
        "    return image_path, f\"Descrição: {description}\"\n",
        "\n",
        "def prepare_download_zip():\n",
        "    valid_paths = [os.path.join(TEMP_DIR, fn) for fn in ANNOTATIONS if ANNOTATIONS[fn].get(\"valid\")]\n",
        "    if not valid_paths: return None\n",
        "    zip_path = \"segmentos_validos.zip\"\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "        for p in valid_paths: zf.write(p, os.path.basename(p))\n",
        "    return zip_path\n",
        "\n",
        "def compare_descriptions_openai(practice_filename, practice_description, correct_annotations):\n",
        "    # --- CORREÇÃO APLICADA AQUI ---\n",
        "    global OPENAI_API_KEY\n",
        "    if not practice_filename or not practice_description:\n",
        "        return \"Por favor, escolha uma imagem e digite uma descrição.\"\n",
        "    if not correct_annotations:\n",
        "        return \"Por favor, exporte o arquivo JSON na primeira aba para carregar as respostas.\"\n",
        "    if not OPENAI_API_KEY or OPENAI_API_KEY == \"COLE_SUA_CHAVE_DA_API_AQUI\":\n",
        "        return \"ERRO: A chave da API da OpenAI não foi definida no código fonte.\"\n",
        "\n",
        "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "    original_description = correct_annotations.get(practice_filename)\n",
        "    if not original_description:\n",
        "        return f\"Não foi encontrada uma anotação para '{practice_filename}' no arquivo JSON.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Você é um assistente avaliador. Compare as duas descrições de uma imagem a seguir, focando no significado semântico.\n",
        "    Descrição Original: \"{original_description}\"\n",
        "    Descrição do Usuário: \"{practice_description}\"\n",
        "\n",
        "    Responda em duas linhas:\n",
        "    1. Na primeira linha, forneça um placar de semelhança de 0 a 100.\n",
        "    2. Na segunda linha, escreva um feedback conciso e amigável em português sobre a comparação.\n",
        "\n",
        "    Exemplo de Resposta:\n",
        "    Placar: 95\n",
        "    Feedback: Excelente! As descrições são semanticamente quase idênticas.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.2,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        ai_response = response.choices[0].message.content\n",
        "        return f\"**Resultado da Comparação (IA):**\\n\\n{ai_response}\\n\\n**Descrição Original:**\\n'{original_description}'\"\n",
        "    except openai.AuthenticationError:\n",
        "        return \"ERRO: A chave da API da OpenAI é inválida ou expirou.\"\n",
        "    except Exception as e:\n",
        "        return f\"ERRO: Ocorreu um erro ao contatar a API da OpenAI: {e}\"\n",
        "\n",
        "# --- CONSTRUÇÃO DA INTERFACE ---\n",
        "with gr.Blocks(theme=gr.themes.Soft(), css=\"footer {display: none !important}\") as demo:\n",
        "    gr.Markdown(\"# 🖼️ Ferramenta de Anotação e Prática Cognitiva com SAM 2\")\n",
        "    segment_paths_state, original_image_state, json_content_state = gr.State([]), gr.State(), gr.State()\n",
        "\n",
        "    with gr.Tabs() as tabs:\n",
        "        with gr.TabItem(\"1. Segmentar e Anotar\", id=0) as annotate_tab:\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    input_image = gr.Image(type=\"pil\", label=\"Carregue sua Imagem\")\n",
        "                    segment_button = gr.Button(\"▶️ Segmentar Imagem\", variant=\"primary\")\n",
        "                    overlay_image_display = gr.Image(label=\"Visualização das Máscaras Válidas\", interactive=False)\n",
        "                with gr.Column(scale=2):\n",
        "                    gallery = gr.Gallery(label=\"Segmentos Gerados\", elem_id=\"gallery\", columns=6, height=\"auto\")\n",
        "            with gr.Group(visible=False) as annotation_box:\n",
        "                with gr.Row():\n",
        "                    selected_image_preview = gr.Image(label=\"Segmento Selecionado\", interactive=False)\n",
        "                    with gr.Column():\n",
        "                        original_filename_hidden, editable_filename_textbox = gr.Textbox(visible=False), gr.Textbox(label=\"Nome do Arquivo\", interactive=True)\n",
        "                        is_valid_checkbox = gr.Checkbox(label=\"Este segmento é válido?\")\n",
        "                        with gr.Group(visible=False) as annotation_fields:\n",
        "                            description_textbox = gr.Textbox(label=\"Descrição\", lines=3)\n",
        "                            save_button = gr.Button(\"💾 Salvar Anotação\", variant=\"secondary\")\n",
        "            status_textbox = gr.Textbox(label=\"Status\", interactive=False)\n",
        "            with gr.Group(visible=False) as download_box:\n",
        "                gr.Markdown(\"---\")\n",
        "                gr.Markdown(\"### Finalizar e Baixar\")\n",
        "                finalize_button = gr.Button(\"✅ Finalizar Anotação\", variant=\"stop\")\n",
        "                with gr.Row():\n",
        "                    prepare_download_button, export_json_button = gr.Button(\"📦 Baixar Imagens (.zip)\"), gr.Button(\"📄 Baixar Anotações (.json)\")\n",
        "                with gr.Row():\n",
        "                    download_zip_output, download_json_output = gr.File(label=\".zip\"), gr.File(label=\".json\")\n",
        "\n",
        "        with gr.TabItem(\"2. Consultar Anotações\", id=1, visible=False) as query_tab:\n",
        "            gr.Markdown(\"### Consultar Descrição de um Segmento\")\n",
        "            gr.Markdown(\"Escolha um dos segmentos finalizados para ver sua descrição.\")\n",
        "            with gr.Row():\n",
        "                query_image_selector = gr.Dropdown(label=\"Escolha um Segmento para Consultar\")\n",
        "                with gr.Column():\n",
        "                    query_image_preview, query_result = gr.Image(label=\"Segmento\"), gr.Textbox(label=\"Resultado\")\n",
        "\n",
        "        with gr.TabItem(\"3. Praticar\", id=2, visible=False) as practice_tab:\n",
        "            gr.Markdown(\"### Praticar Memória e Cognição com IA\")\n",
        "            gr.Markdown(\"Escolha um segmento e teste sua memória. A comparação será feita pela IA da OpenAI.\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    practice_image_selector = gr.Dropdown(label=\"Escolha um Segmento para Praticar\")\n",
        "                    practice_image_display = gr.Image(label=\"Imagem Selecionada\", interactive=False)\n",
        "                with gr.Column():\n",
        "                    # O campo da chave foi removido da interface\n",
        "                    practice_description_input = gr.Textbox(label=\"Descreva a imagem de memória\", lines=4)\n",
        "                    compare_button = gr.Button(\"🧠 Comparar com IA\", variant=\"primary\")\n",
        "                    comparison_result_display = gr.Markdown(label=\"Resultado da Comparação\")\n",
        "\n",
        "    # --- LÓGICA DOS EVENTOS ---\n",
        "    segment_button.click(fn=segment_image, inputs=input_image, outputs=[gallery, segment_paths_state, original_image_state, annotation_box, download_box, query_tab, practice_tab]).then(fn=lambda x: x, inputs=input_image, outputs=overlay_image_display)\n",
        "    gallery.select(fn=get_annotation_data, inputs=[segment_paths_state], outputs=[selected_image_preview, original_filename_hidden, editable_filename_textbox, is_valid_checkbox, description_textbox, annotation_fields])\n",
        "    is_valid_checkbox.change(fn=handle_valid_checkbox, inputs=[is_valid_checkbox, original_filename_hidden, original_image_state], outputs=[annotation_fields, status_textbox, overlay_image_display])\n",
        "    save_button.click(fn=save_annotation, inputs=[original_filename_hidden, editable_filename_textbox, description_textbox, original_image_state], outputs=[status_textbox, gallery, segment_paths_state, overlay_image_display])\n",
        "    finalize_button.click(fn=finalize_annotations, inputs=None, outputs=[query_tab, practice_tab, practice_image_selector, query_image_selector, status_textbox])\n",
        "\n",
        "    query_image_selector.change(fn=show_and_get_description, inputs=query_image_selector, outputs=[query_image_preview, query_result])\n",
        "    practice_image_selector.change(fn=lambda x: os.path.join(TEMP_DIR, x) if x else None, inputs=practice_image_selector, outputs=practice_image_display)\n",
        "\n",
        "    prepare_download_button.click(fn=prepare_download_zip, inputs=None, outputs=download_zip_output)\n",
        "    export_json_button.click(fn=export_annotations_to_json, inputs=None, outputs=[download_json_output, json_content_state])\n",
        "    # A chave da API não é mais passada como um input da interface\n",
        "    compare_button.click(fn=compare_descriptions_openai, inputs=[practice_image_selector, practice_description_input, json_content_state], outputs=comparison_result_display)\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "pjXFd8EjpMEp",
        "outputId": "d5c49af6-9303-4de3-e419-ba9f78ba96f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://cecd9a3791c7615c2c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cecd9a3791c7615c2c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando segmentação...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sam2/sam2_image_predictor.py:431: UserWarning: /usr/local/lib/python3.11/dist-packages/sam2/_C.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
            "\n",
            "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
            "  masks = self._transforms.postprocess_masks(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando segmentação...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sam2/sam2_image_predictor.py:431: UserWarning: /usr/local/lib/python3.11/dist-packages/sam2/_C.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
            "\n",
            "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
            "  masks = self._transforms.postprocess_masks(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://cecd9a3791c7615c2c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}